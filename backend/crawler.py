import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import time
import re
from typing import Dict, List, Optional
from datetime import datetime



def setup_driver():
    """í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •"""
    try:
        print("\nğŸš— ChromeDriver ì´ˆê¸°í™” ì¤‘...")
        
        options = uc.ChromeOptions()
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36')
        
        # Chrome 140 ë²„ì „ì— ë§ëŠ” ë“œë¼ì´ë²„
        driver = uc.Chrome(
            options=options, 
            version_main=140,
            use_subprocess=True
        )
        
        driver.set_page_load_timeout(30)
        
        print("âœ… ChromeDriver ì´ˆê¸°í™” ì™„ë£Œ (Chrome v140)")
        return driver
        
    except Exception as e:
        print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

def clean_text(s):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not s:
        return None
    cleaned = re.sub(r'\s+', ' ', s.replace('\xa0', ' ').replace('\u200b', '')).strip()
    return cleaned if cleaned else None

def parse_release_date(date_text):
    """ê°œë´‰ì¼ í…ìŠ¤íŠ¸ë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_text:
        return None
    
    date_text = date_text.strip()
    
    try:
        # YYYY.MM.DD í˜•ì‹
        match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # YYYY-MM-DD í˜•ì‹
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return None
    except Exception:
        return None

def search_and_crawl_movie(movie_title: str) -> Optional[Dict]:
    """ì˜í™” ì œëª©ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ì²« ë²ˆì§¸ ê²°ê³¼ í¬ë¡¤ë§"""
    driver = None
    
    try:
        print(f"\n{'='*60}")
        print(f"í¬ë¡¤ë§ ì‹œì‘: '{movie_title}'")
        print(f"{'='*60}")
        
        driver = setup_driver()
        if not driver:
            print("âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            return None
        
        # ê²€ìƒ‰ì–´ ì •ì œ
        search_query = movie_title.strip()
        
        if len(search_query) < 2:
            print(f"âŒ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: '{search_query}'")
            return None
        
        # Cine21 ê²€ìƒ‰ (URL ì¸ì½”ë”©)
        encoded_query = quote(search_query)
        search_url = f"https://cine21.com/search/movie?query={encoded_query}"
        
        print(f"ğŸ” ê²€ìƒ‰ URL: {search_url}")
        driver.get(search_url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("â±ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        
        time.sleep(3)
        
        # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ë””ë²„ê¹…: í˜ì´ì§€ ì œëª© í™•ì¸
        page_title = soup.select_one('title')
        if page_title:
            print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title.get_text()}")
        
        # ì—¬ëŸ¬ ì„ íƒìë¡œ ì˜í™” ë§í¬ ì°¾ê¸°
        movie_links = []
        
        # ì‹œë„ 1: ê¸°ë³¸ ì„ íƒì
        movie_links = soup.select('a[href*="/movie/info/"]')
        print(f"  ì‹œë„ 1 (ê¸°ë³¸): {len(movie_links)}ê°œ ë°œê²¬")
        
        # ì‹œë„ 2: ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ
        if not movie_links:
            movie_links = soup.select('.search_result a[href*="/movie/"]')
            print(f"  ì‹œë„ 2 (ê²€ìƒ‰ ê²°ê³¼): {len(movie_links)}ê°œ ë°œê²¬")
        
        # ì‹œë„ 3: ì˜í™” ëª©ë¡
        if not movie_links:
            movie_links = soup.select('.movie_list a[href*="/movie/"]')
            print(f"  ì‹œë„ 3 (ì˜í™” ëª©ë¡): {len(movie_links)}ê°œ ë°œê²¬")
        
        # ì‹œë„ 4: ëª¨ë“  ì˜í™” ê´€ë ¨ ë§í¬
        if not movie_links:
            all_links = soup.find_all('a', href=True)
            movie_links = [link for link in all_links if '/movie/' in link.get('href', '')]
            print(f"  ì‹œë„ 4 (ì „ì²´ ë§í¬): {len(movie_links)}ê°œ ë°œê²¬")
        
        if not movie_links:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print("\në””ë²„ê¹… ì •ë³´:")
            print(f"  - í˜ì´ì§€ ê¸¸ì´: {len(driver.page_source)} ë¬¸ì")
            
            # ê²€ìƒ‰ ê²°ê³¼ ë©”ì‹œì§€ í™•ì¸
            no_result = soup.select_one('.no_result, .empty, .search_empty')
            if no_result:
                print(f"  - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë©”ì‹œì§€: {no_result.get_text()}")
            
            return None
        
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ {len(movie_links)}ê°œ ë°œê²¬")
        
        # ì²« ë²ˆì§¸ ì˜í™” ì„ íƒ
        first_link = movie_links[0].get('href')
        movie_url = urljoin("https://cine21.com", first_link)
        
        # ì˜í™” ì œëª© í™•ì¸ (ê°€ëŠ¥í•˜ë©´)
        first_movie_title = None
        title_elem = movie_links[0].select_one('.title, h3, h4, p.title')
        if title_elem:
            first_movie_title = clean_text(title_elem.get_text())
        
        if first_movie_title:
            print(f"ğŸ¬ ì„ íƒëœ ì˜í™”: {first_movie_title}")
        
        print(f"ğŸ”— ì˜í™” URL: {movie_url}")
        
        # ì˜í™”ID ì¶”ì¶œ
        movie_id_match = re.search(r'movie_id=(\d+)', movie_url)
        if not movie_id_match:
            print("âŒ ì˜í™” IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        movie_id = movie_id_match.group(1)
        print(f"ğŸ†” ì˜í™” ID: {movie_id}")
        
        # ìƒì„¸ í˜ì´ì§€ ì´ë™
        print(f"\nìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        driver.get(movie_url)
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("â±ï¸ ìƒì„¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        
        time.sleep(3)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì˜í™” ì •ë³´ ì´ˆê¸°í™”
        movie_data = {
            'ì˜í™”ì´ë¦„': None,
            'ì´ë¯¸ì§€URL': None,
            'ì˜í™”ID': movie_id,
            'ì „ë¬¸ê°€ë³„ì ': None,
            'ê´€ê°ë³„ì ': None,
            'ìƒì˜ì‹œê°„(ë¶„)': None,
            'ê°œë´‰ì¼': None,
            'ì¥ë¥´': None,
            'êµ­ê°€': None,
            'ê°ë…': None,
            'ì¶œì—°': None,
            'ì‹œë†‰ì‹œìŠ¤': None,
            'ì „ë¬¸ê°€ì´ë¦„': None,
            'ì „ë¬¸ê°€ë³„ì ìƒì„¸': None,
            'ì „ë¬¸ê°€ë‚´ìš©': None,
            'ê´€ë ¨ê¸°ì‚¬1': None,
            'ê´€ë ¨ê¸°ì‚¬2': None,
            'ê´€ê°ë³„ì ìƒì„¸': None,
            'ê´€ê°ë¦¬ë·°': None
        }
        
        # 1. ì œëª© - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¨ ì œëª© ìš°ì„  ì‚¬ìš©
        if first_movie_title:
            movie_data['ì˜í™”ì´ë¦„'] = first_movie_title
            print(f"  âœ“ ì œëª©: {movie_data['ì˜í™”ì´ë¦„']}")
        else:
            # ëŒ€ì²´: í˜ì´ì§€ì—ì„œ ì°¾ê¸°
            title_elem = soup.select_one('section.sect_movie_detail h1, div.movie_detail h1, main h1.movie_title')
            if title_elem:
                movie_data['ì˜í™”ì´ë¦„'] = clean_text(title_elem.get_text())
                print(f"  âœ“ ì œëª© (í˜ì´ì§€): {movie_data['ì˜í™”ì´ë¦„']}")
            else:
                page_title = soup.select_one('title')
                if page_title:
                    title_text = page_title.get_text()
                    if '<' in title_text:
                        movie_data['ì˜í™”ì´ë¦„'] = clean_text(title_text.split('<')[0])
                        print(f"  âœ“ ì œëª© (titleíƒœê·¸): {movie_data['ì˜í™”ì´ë¦„']}")
        
        # 2. í¬ìŠ¤í„° ì´ë¯¸ì§€
        poster_elem = soup.select_one('img.poster, .movie_poster img, .poster_wrap img')
        if poster_elem:
            img_url = poster_elem.get('src') or poster_elem.get('data-src')
            if img_url and 'noimg' not in img_url:
                movie_data['ì´ë¯¸ì§€URL'] = urljoin("https://cine21.com", img_url)
                print(f"  âœ“ í¬ìŠ¤í„°: {movie_data['ì´ë¯¸ì§€URL'][:50]}...")
        
        # 3. ë³„ì 
        star_box = soup.select_one('div.movie_detail_star_box_wrap, .star_box')
        if star_box:
            # ì „ë¬¸ê°€ ë³„ì 
            expert_div = star_box.select_one('div.star_cine21, .star_expert')
            if expert_div:
                expert_num = expert_div.select_one('p.num, .rating_num')
                if expert_num:
                    expert_text = expert_num.get_text(strip=True)
                    expert_match = re.search(r'(\d+(?:\.\d+)?)', expert_text)
                    if expert_match:
                        movie_data['ì „ë¬¸ê°€ë³„ì '] = float(expert_match.group(1))
                        print(f"  âœ“ ì „ë¬¸ê°€ë³„ì : {movie_data['ì „ë¬¸ê°€ë³„ì ']}")
            
            # ê´€ê° ë³„ì 
            netizen_div = star_box.select_one('div.star_netizen, .star_audience')
            if netizen_div:
                netizen_num = netizen_div.select_one('p.num, .rating_num')
                if netizen_num:
                    netizen_text = netizen_num.get_text(strip=True)
                    netizen_match = re.search(r'(\d+(?:\.\d+)?)', netizen_text)
                    if netizen_match:
                        movie_data['ê´€ê°ë³„ì '] = float(netizen_match.group(1))
                        print(f"  âœ“ ê´€ê°ë³„ì : {movie_data['ê´€ê°ë³„ì ']}")
        
        # 4. ê¸°ë³¸ ì •ë³´
        info_list = soup.select_one('ul.info_list, .movie_info')
        if info_list:
            list_items = info_list.select('li')
            for li in list_items:
                title_elem = li.select_one('p.title, .info_title')
                if not title_elem:
                    continue
                
                title_text = title_elem.get_text(strip=True)
                li_text = li.get_text()
                content = li_text.replace(title_text, '', 1).strip()
                content = clean_text(content)
                
                if title_text == 'ê°œë´‰':
                    movie_data['ê°œë´‰ì¼'] = parse_release_date(content)
                    if movie_data['ê°œë´‰ì¼']:
                        print(f"  âœ“ ê°œë´‰ì¼: {movie_data['ê°œë´‰ì¼']}")
                elif title_text == 'ì‹œê°„':
                    runtime_match = re.search(r'(\d{1,3})\s*ë¶„', content)
                    if runtime_match:
                        movie_data['ìƒì˜ì‹œê°„(ë¶„)'] = int(runtime_match.group(1))
                        print(f"  âœ“ ìƒì˜ì‹œê°„: {movie_data['ìƒì˜ì‹œê°„(ë¶„)']}ë¶„")
                elif title_text == 'ì¥ë¥´':
                    movie_data['ì¥ë¥´'] = content
                    print(f"  âœ“ ì¥ë¥´: {movie_data['ì¥ë¥´']}")
                elif title_text == 'êµ­ê°€':
                    movie_data['êµ­ê°€'] = content
                    print(f"  âœ“ êµ­ê°€: {movie_data['êµ­ê°€']}")
                elif title_text == 'ê°ë…':
                    movie_data['ê°ë…'] = content
                    print(f"  âœ“ ê°ë…: {movie_data['ê°ë…']}")
                elif title_text == 'ì¶œì—°':
                    movie_data['ì¶œì—°'] = content
                    print(f"  âœ“ ì¶œì—°: {movie_data['ì¶œì—°'][:50]}...")
        
        # 5. ì‹œë†‰ì‹œìŠ¤ (XPath ì‚¬ìš©)
        try:
            synopsis_xpath = '/html/body/div[4]/main/div/div/div/div/section[1]/div/div'
            synopsis_elem = driver.find_element(By.XPATH, synopsis_xpath)
            synopsis_text = synopsis_elem.text.strip()
            if synopsis_text:
                movie_data['ì‹œë†‰ì‹œìŠ¤'] = synopsis_text
                print(f"  âœ“ ì‹œë†‰ì‹œìŠ¤: {synopsis_text[:50]}...")
        except NoSuchElementException:
            # XPath ì‹¤íŒ¨ì‹œ CSS ì„ íƒìë¡œ ì‹œë„
            synopsis_elem = soup.select_one('.synopsis, .movie_synopsis, .story')
            if synopsis_elem:
                movie_data['ì‹œë†‰ì‹œìŠ¤'] = clean_text(synopsis_elem.get_text())
                print(f"  âœ“ ì‹œë†‰ì‹œìŠ¤ (CSS): {movie_data['ì‹œë†‰ì‹œìŠ¤'][:50]}...")
        except Exception as e:
            print(f"  âš ï¸ ì‹œë†‰ì‹œìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 6. ì „ë¬¸ê°€ ë¦¬ë·° (ìµœëŒ€ 5ê°œ)
        try:
            expert_names = []
            expert_ratings = []
            expert_contents = []
            
            expert_base_xpath = '/html/body/div[4]/main/div/div/div/div/section[4]/ul/li'
            expert_elements = driver.find_elements(By.XPATH, expert_base_xpath)
            
            for i, li_elem in enumerate(expert_elements[:5], 1):
                try:
                    name_xpath = f'{expert_base_xpath}[{i}]/div[1]/p'
                    name = driver.find_element(By.XPATH, name_xpath).text.strip()
                    
                    rating_xpath = f'{expert_base_xpath}[{i}]/div[1]/div/p'
                    rating = driver.find_element(By.XPATH, rating_xpath).text.strip()
                    
                    content_xpath = f'{expert_base_xpath}[{i}]/div[2]'
                    content = driver.find_element(By.XPATH, content_xpath).text.strip()
                    
                    if name:
                        expert_names.append(name)
                        expert_ratings.append(rating)
                        expert_contents.append(content[:500])  # ìµœëŒ€ 500ì
                except:
                    continue
            
            if expert_names:
                movie_data['ì „ë¬¸ê°€ì´ë¦„'] = ' | '.join(expert_names)
                movie_data['ì „ë¬¸ê°€ë³„ì ìƒì„¸'] = ' | '.join(expert_ratings)
                movie_data['ì „ë¬¸ê°€ë‚´ìš©'] = ' | '.join(expert_contents)
                print(f"  âœ“ ì „ë¬¸ê°€ ë¦¬ë·°: {len(expert_names)}ê°œ")
        except Exception as e:
            print(f"  âš ï¸ ì „ë¬¸ê°€ ë¦¬ë·° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 7. ê´€ë ¨ ê¸°ì‚¬ (ìµœëŒ€ 2ê°œ)
        try:
            article_xpath_1 = '/html/body/div[4]/main/div/div/div/div/section[5]/ul/li[1]/a'
            article_xpath_2 = '/html/body/div[4]/main/div/div/div/div/section[5]/ul/li[2]/a'
            
            try:
                article1_elem = driver.find_element(By.XPATH, article_xpath_1)
                article1_text = article1_elem.text.strip()
                article1_href = article1_elem.get_attribute('href')
                if article1_text and article1_href:
                    movie_data['ê´€ë ¨ê¸°ì‚¬1'] = f"{article1_text} | {article1_href}"
                    print(f"  âœ“ ê´€ë ¨ê¸°ì‚¬1: {article1_text[:30]}...")
            except:
                pass
            
            try:
                article2_elem = driver.find_element(By.XPATH, article_xpath_2)
                article2_text = article2_elem.text.strip()
                article2_href = article2_elem.get_attribute('href')
                if article2_text and article2_href:
                    movie_data['ê´€ë ¨ê¸°ì‚¬2'] = f"{article2_text} | {article2_href}"
                    print(f"  âœ“ ê´€ë ¨ê¸°ì‚¬2: {article2_text[:30]}...")
            except:
                pass
        except Exception as e:
            print(f"  âš ï¸ ê´€ë ¨ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {movie_data['ì˜í™”ì´ë¦„']} (ID: {movie_id})")
        print(f"{'='*60}\n")
        
        return movie_data
        
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        if driver:
            try:
                driver.quit()
                print("ğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass


def search_movies_on_cine21(query: str) -> list:
    """Cine21ì—ì„œ ì˜í™” ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ ëª©ë¡ ë°˜í™˜"""
    driver = None
    
    try:
        print(f"\n{'='*60}")
        print(f"ì˜í™” ê²€ìƒ‰: '{query}'")
        print(f"{'='*60}")
        
        driver = setup_driver()
        if not driver:
            print("âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            return []
        
        # ê²€ìƒ‰ì–´ ì •ì œ
        search_query = query.strip()
        
        if len(search_query) < 2:
            print(f"âŒ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: '{search_query}'")
            return []
        
        # Cine21 ê²€ìƒ‰
        encoded_query = quote(search_query)
        search_url = f"https://cine21.com/search/movie/?query={encoded_query}"
        
        print(f"ğŸ” ê²€ìƒ‰ URL: {search_url}")
        driver.get(search_url)
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("â±ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        
        time.sleep(3)
        
        # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± - sql_0929_final.pyì˜ collect_movie_list ë¡œì§ ì°¸ê³ 
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        print(f"ğŸ“„ í˜ì´ì§€ íƒ€ì´í‹€: {soup.title.string if soup.title else 'None'}")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        # ê²€ìƒ‰ í˜ì´ì§€ëŠ” list_with_leftthumb ì‚¬ìš©
        container = soup.select_one('.list_with_leftthumb')
        
        if not container:
            # ëŒ€ì²´: ì¼ë°˜ ëª©ë¡ í˜ì´ì§€
            container = soup.select_one('#point_list_holder')
        
        if not container:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        
        # ì˜í™” ì¹´ë“œ ì°¾ê¸°
        # ê²€ìƒ‰ í˜ì´ì§€ëŠ” list_with_leftthumb_item ì‚¬ìš©
        cards = container.select('div.list_with_leftthumb_item')
        
        if not cards:
            # ëŒ€ì²´: ì¼ë°˜ ëª©ë¡ í˜ì´ì§€ ì…€ë ‰í„°
            cards = container.select('div.list_with_upthumb_item')
        
        if not cards:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []
        
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ {len(cards)}ê°œ ë°œê²¬")
        
        # ì˜í™” ì •ë³´ ìˆ˜ì§‘
        results = []
        seen_ids = set()

        for idx, card in enumerate(cards[:20], 1):  # ìµœëŒ€ 20ê°œ
            try:
                # ì œëª© ì¶”ì¶œ - sql_0929_final.pyì™€ ë™ì¼í•œ ë°©ì‹
                title_elem = card.select_one('p.title')
                title = title_elem.get_text(strip=True) if title_elem else None
                title = clean_text(title)
                
                if not title:
                    print(f"  âš ï¸ í•­ëª© {idx}: ì œëª© ì—†ìŒ - ìŠ¤í‚µ")
                    continue
                
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                img_elem = card.select_one('div.img_wrap img')
                poster_url = None
                if img_elem:
                    poster_url = img_elem.get('src') or img_elem.get('data-src')
                    if poster_url and not poster_url.startswith('http'):
                        poster_url = urljoin("https://cine21.com", poster_url)
                    # noimg íŒ¨í„´ ì²´í¬
                    if poster_url and 'noimg' in poster_url:
                        poster_url = None
                
                # ìƒì„¸ URL ì¶”ì¶œ
                link_elem = card.select_one('a[href*="/movie/info/"]')
                detail_url = None
                movie_id = None
                
                if link_elem:
                    href = link_elem.get('href')
                    if href:
                        detail_url = urljoin("https://cine21.com", href) if not href.startswith('http') else href
                        # movie_id ì¶”ì¶œ
                        movie_id_match = re.search(r'movie_id=(\d+)', detail_url)
                        if movie_id_match:
                            movie_id = movie_id_match.group(1)
                
                # ì˜í™”ID í•„ìˆ˜ ì²´í¬
                if not movie_id:
                    print(f"  âš ï¸ í•­ëª© {idx}: ì˜í™”ID ì—†ìŒ (ì œëª©={title}) - ìŠ¤í‚µ")
                    continue
                
                # ì¤‘ë³µ ì œê±°
                if movie_id in seen_ids:
                    continue
                seen_ids.add(movie_id)
                
                # ê²°ê³¼ ì¶”ê°€
                movie_info = {
                    'movie_id': movie_id,
                    'title': title,
                    'poster_url': poster_url,
                    'detail_url': detail_url
                }
                
                results.append(movie_info)
                
                print(f"  âœ“ {idx}. [{movie_id}] {title}")
                
            except Exception as e:
                print(f"  âŒ í•­ëª© {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\n{'='*60}")
        print(f"âœ… ì´ {len(results)}ê°œì˜ ì˜í™” ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"{'='*60}\n")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
        
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def crawl_movie_by_id_direct(movie_id: str) -> Optional[Dict]:
    """ì˜í™” IDë¡œ ì§ì ‘ í¬ë¡¤ë§ (ê²€ìƒ‰ ê³¼ì • ìƒëµ)"""
    driver = None
    
    try:
        print(f"\n{'='*60}")
        print(f"ì˜í™” í¬ë¡¤ë§: ID {movie_id}")
        print(f"{'='*60}")
        
        driver = setup_driver()
        if not driver:
            print("âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            return None
        
        # ì˜í™” ìƒì„¸ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™
        movie_url = f"https://cine21.com/movie/info?movie_id={movie_id}"
        
        print(f"ğŸ”— ì˜í™” URL: {movie_url}")
        driver.get(movie_url)
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("â±ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        
        time.sleep(3)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì˜í™” ì •ë³´ ì´ˆê¸°í™”
        movie_data = {
            'ì˜í™”ì´ë¦„': None,
            'ì´ë¯¸ì§€URL': None,
            'ì˜í™”ID': movie_id,
            'ì „ë¬¸ê°€ë³„ì ': None,
            'ê´€ê°ë³„ì ': None,
            'ìƒì˜ì‹œê°„(ë¶„)': None,
            'ê°œë´‰ì¼': None,
            'ì¥ë¥´': None,
            'êµ­ê°€': None,
            'ê°ë…': None,
            'ì¶œì—°': None,
            'ì‹œë†‰ì‹œìŠ¤': None,
            'ì „ë¬¸ê°€ì´ë¦„': None,
            'ì „ë¬¸ê°€ë³„ì ìƒì„¸': None,
            'ì „ë¬¸ê°€ë‚´ìš©': None,
            'ê´€ë ¨ê¸°ì‚¬1': None,
            'ê´€ë ¨ê¸°ì‚¬2': None,
            'ê´€ê°ë³„ì ìƒì„¸': None,
            'ê´€ê°ë¦¬ë·°': None
        }
        
        # 1. ì œëª© - í˜ì´ì§€ title íƒœê·¸ì—ì„œ ì¶”ì¶œ
        page_title = soup.select_one('title')
        if page_title:
            title_text = page_title.get_text()
            if '<' in title_text:
                movie_data['ì˜í™”ì´ë¦„'] = clean_text(title_text.split('<')[0])
            else:
                movie_data['ì˜í™”ì´ë¦„'] = clean_text(title_text)
            
            if movie_data['ì˜í™”ì´ë¦„']:
                print(f"  âœ“ ì œëª©: {movie_data['ì˜í™”ì´ë¦„']}")
        
        # ì œëª©ì„ ëª» ì°¾ì€ ê²½ìš° HTMLì—ì„œ ì‹œë„
        if not movie_data['ì˜í™”ì´ë¦„']:
            title_elem = soup.select_one('section.sect_movie_detail h1, div.movie_detail h1, main h1.movie_title')
            if title_elem:
                movie_data['ì˜í™”ì´ë¦„'] = clean_text(title_elem.get_text())
                print(f"  âœ“ ì œëª© (HTML): {movie_data['ì˜í™”ì´ë¦„']}")
        
        
        
        # 2. í¬ìŠ¤í„° ì´ë¯¸ì§€ ì¶”ì¶œ
        try:
            # ë°©ë²• 1: í¬ìŠ¤í„° ì˜ì—­ì—ì„œ ì°¾ê¸°
            poster_elem = soup.select_one('div.poster img, .movie_poster img, .poster_wrap img')
            if poster_elem:
                poster_url = poster_elem.get('src') or poster_elem.get('data-src')
                if poster_url:
                    if not poster_url.startswith('http'):
                        poster_url = urljoin("https://cine21.com", poster_url)
                    # noimg ì²´í¬
                    if 'noimg' not in poster_url.lower():
                        movie_data['ì´ë¯¸ì§€URL'] = poster_url
                        print(f"  âœ“ í¬ìŠ¤í„°: {poster_url[:60]}...")
            
            # ë°©ë²• 2: ë©”íƒ€ og:image
            if not movie_data['ì´ë¯¸ì§€URL']:
                meta_img = soup.select_one('meta[property="og:image"]')
                if meta_img:
                    poster_url = meta_img.get('content')
                    if poster_url and 'noimg' not in poster_url.lower():
                        movie_data['ì´ë¯¸ì§€URL'] = poster_url
                        print(f"  âœ“ í¬ìŠ¤í„° (og:image): {poster_url[:60]}...")
                        
        except Exception as e:
            print(f"  âš ï¸ í¬ìŠ¤í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        # 3. ë³„ì 
        star_box = soup.select_one('div.movie_detail_star_box_wrap, .star_box')
        if star_box:
            # ì „ë¬¸ê°€ ë³„ì 
            expert_div = star_box.select_one('div.star_cine21, .star_expert')
            if expert_div:
                expert_num = expert_div.select_one('p.num, .rating_num')
                if expert_num:
                    expert_text = expert_num.get_text(strip=True)
                    expert_match = re.search(r'(\d+(?:\.\d+)?)', expert_text)
                    if expert_match:
                        movie_data['ì „ë¬¸ê°€ë³„ì '] = float(expert_match.group(1))
                        print(f"  âœ“ ì „ë¬¸ê°€ë³„ì : {movie_data['ì „ë¬¸ê°€ë³„ì ']}")
            
            # ê´€ê° ë³„ì 
            netizen_div = star_box.select_one('div.star_netizen, .star_audience')
            if netizen_div:
                netizen_num = netizen_div.select_one('p.num, .rating_num')
                if netizen_num:
                    netizen_text = netizen_num.get_text(strip=True)
                    netizen_match = re.search(r'(\d+(?:\.\d+)?)', netizen_text)
                    if netizen_match:
                        movie_data['ê´€ê°ë³„ì '] = float(netizen_match.group(1))
                        print(f"  âœ“ ê´€ê°ë³„ì : {movie_data['ê´€ê°ë³„ì ']}")
        
        # 4. ê¸°ë³¸ ì •ë³´ (ì´í•˜ ë™ì¼ - ìœ„ ì½”ë“œì™€ ê°™ìŒ)
        info_list = soup.select_one('ul.info_list, .movie_info')
        if info_list:
            list_items = info_list.select('li')
            for li in list_items:
                title_elem = li.select_one('p.title, .info_title')
                if not title_elem:
                    continue
                
                title_text = title_elem.get_text(strip=True)
                li_text = li.get_text()
                content = li_text.replace(title_text, '', 1).strip()
                content = clean_text(content)
                
                if title_text == 'ê°œë´‰':
                    movie_data['ê°œë´‰ì¼'] = parse_release_date(content)
                    if movie_data['ê°œë´‰ì¼']:
                        print(f"  âœ“ ê°œë´‰ì¼: {movie_data['ê°œë´‰ì¼']}")
                elif title_text == 'ì‹œê°„':
                    runtime_match = re.search(r'(\d{1,3})\s*ë¶„', content)
                    if runtime_match:
                        movie_data['ìƒì˜ì‹œê°„(ë¶„)'] = int(runtime_match.group(1))
                        print(f"  âœ“ ìƒì˜ì‹œê°„: {movie_data['ìƒì˜ì‹œê°„(ë¶„)']}ë¶„")
                elif title_text == 'ì¥ë¥´':
                    movie_data['ì¥ë¥´'] = content
                    print(f"  âœ“ ì¥ë¥´: {movie_data['ì¥ë¥´']}")
                elif title_text == 'êµ­ê°€':
                    movie_data['êµ­ê°€'] = content
                    print(f"  âœ“ êµ­ê°€: {movie_data['êµ­ê°€']}")
                elif title_text == 'ê°ë…':
                    movie_data['ê°ë…'] = content
                    print(f"  âœ“ ê°ë…: {movie_data['ê°ë…']}")
                elif title_text == 'ì¶œì—°':
                    movie_data['ì¶œì—°'] = content
                    print(f"  âœ“ ì¶œì—°: {movie_data['ì¶œì—°'][:50]}...")
        
        # 5. ì‹œë†‰ì‹œìŠ¤
        try:
            synopsis_xpath = '/html/body/div[4]/main/div/div/div/div/section[1]/div/div'
            synopsis_elem = driver.find_element(By.XPATH, synopsis_xpath)
            synopsis_text = synopsis_elem.text.strip()
            if synopsis_text:
                movie_data['ì‹œë†‰ì‹œìŠ¤'] = synopsis_text
                print(f"  âœ“ ì‹œë†‰ì‹œìŠ¤: {synopsis_text[:50]}...")
        except:
            synopsis_elem = soup.select_one('.synopsis, .movie_synopsis, .story')
            if synopsis_elem:
                movie_data['ì‹œë†‰ì‹œìŠ¤'] = clean_text(synopsis_elem.get_text())
        
        # 6. ì „ë¬¸ê°€ ë¦¬ë·°
        try:
            expert_names = []
            expert_ratings = []
            expert_contents = []
            
            expert_base_xpath = '/html/body/div[4]/main/div/div/div/div/section[4]/ul/li'
            expert_elements = driver.find_elements(By.XPATH, expert_base_xpath)
            
            for i, li_elem in enumerate(expert_elements[:5], 1):
                try:
                    name_xpath = f'{expert_base_xpath}[{i}]/div[1]/p'
                    name = driver.find_element(By.XPATH, name_xpath).text.strip()
                    
                    rating_xpath = f'{expert_base_xpath}[{i}]/div[1]/div/p'
                    rating = driver.find_element(By.XPATH, rating_xpath).text.strip()
                    
                    content_xpath = f'{expert_base_xpath}[{i}]/div[2]'
                    content = driver.find_element(By.XPATH, content_xpath).text.strip()
                    
                    if name:
                        expert_names.append(name)
                        expert_ratings.append(rating)
                        expert_contents.append(content[:500])
                except:
                    continue
            
            if expert_names:
                movie_data['ì „ë¬¸ê°€ì´ë¦„'] = ' | '.join(expert_names)
                movie_data['ì „ë¬¸ê°€ë³„ì ìƒì„¸'] = ' | '.join(expert_ratings)
                movie_data['ì „ë¬¸ê°€ë‚´ìš©'] = ' | '.join(expert_contents)
                print(f"  âœ“ ì „ë¬¸ê°€ ë¦¬ë·°: {len(expert_names)}ê°œ")
        except:
            pass
        
        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {movie_data['ì˜í™”ì´ë¦„']} (ID: {movie_id})")
        print(f"{'='*60}\n")
        
        return movie_data
        
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        if driver:
            try:
                driver.quit()
                print("ğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass